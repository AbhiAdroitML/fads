{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from gensim.models import word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "           name                                            comment  \\\n0     schmoyoho  Stream the track! https://open.spotify.com/alb...   \n1       j â€¢ d i          youtube bros tho ğŸ˜¸ğŸ¤ğŸ¼ğŸ‘ğŸ»ğŸ‘ğŸ»ğŸ‘ğŸ»ğŸ‘ğŸ»ğŸ‘ğŸ»ğŸ‘ğŸ»ğŸ‘ğŸ»ğŸ‘ğŸ»  lol   \n2  Lonn Roberts                 this songs been in my head all day   \n3        Weapon                           Peak internet glory days   \n4           sbi  So my question. Did they find the rapist is Li...   \n\n           published_at  likes Reply Count                   parent_id  \\\n0  2018-07-12T18:42:05Z  13998         189  UgywoWu8TaqkOzGi8HF4AaABAg   \n1  2022-05-15T14:29:59Z      0           0  UgyfeNHiP8lcSxAr2qB4AaABAg   \n2  2022-05-15T03:42:37Z      0           0  UgxHvi5Eu_iPCzenYkN4AaABAg   \n3  2022-05-14T19:55:01Z      1           0  UgwzacFLCUiYgSpm2554AaABAg   \n4  2022-05-14T16:47:53Z      0           0  UgxxcUMzPyuHt7dpoqB4AaABAg   \n\n  is_parent    video_link date  reply_count  ... impurity  year  month  \\\n0       Yes  Bed Intruder  NaT          NaN  ...      0.0  2018     07   \n1       Yes  Bed Intruder  NaT          NaN  ...      0.0  2022     05   \n2       Yes  Bed Intruder  NaT          NaN  ...      0.0  2022     05   \n3       Yes  Bed Intruder  NaT          NaN  ...      0.0  2022     05   \n4       Yes  Bed Intruder  NaT          NaN  ...      0.0  2022     05   \n\n                                              lemmas  \\\n0  [stream, track, httpsopen.spotify.comalbum6vw3...   \n1                           [youtube, bro, tho, lol]   \n2                                  [song, head, day]   \n3                       [peak, internet, glory, day]   \n4            [question, find, rapist, lincoln, park]   \n\n                 adjs_verbs  \\\n0  [stream, proceed, great]   \n1                        []   \n2                        []   \n3                        []   \n4                    [find]   \n\n                                               nouns  \\\n0  [track, httpsopen.spotify.comalbum6vw3oxeqedqc...   \n1                           [youtube, bro, tho, lol]   \n2                                  [song, head, day]   \n3                       [peak, internet, glory, day]   \n4                  [question, rapist, lincoln, park]   \n\n                                        noun_phrases adj_noun_phrases  \\\n0  [track_httpsopen.spotify.comalbum6vw3oxeqedqci...     [great_gift]   \n1                                      [youtube_bro]               []   \n2                                                 []               []   \n3    [internet_glory, internet_glory_day, glory_day]               []   \n4                                                 []               []   \n\n  Verb_noun_phrases                                       lemmasNgrams  \n0                []  [stream, track, httpsopen.spotify.comalbum6vw3...  \n1                []                           [youtube, bro, tho, lol]  \n2                []                                   [song, head_day]  \n3                []                         [peak_internet, glory_day]  \n4                []              [question_find, rapist_lincoln, park]  \n\n[5 rows x 22 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>name</th>\n      <th>comment</th>\n      <th>published_at</th>\n      <th>likes</th>\n      <th>Reply Count</th>\n      <th>parent_id</th>\n      <th>is_parent</th>\n      <th>video_link</th>\n      <th>date</th>\n      <th>reply_count</th>\n      <th>...</th>\n      <th>impurity</th>\n      <th>year</th>\n      <th>month</th>\n      <th>lemmas</th>\n      <th>adjs_verbs</th>\n      <th>nouns</th>\n      <th>noun_phrases</th>\n      <th>adj_noun_phrases</th>\n      <th>Verb_noun_phrases</th>\n      <th>lemmasNgrams</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>schmoyoho</td>\n      <td>Stream the track! https://open.spotify.com/alb...</td>\n      <td>2018-07-12T18:42:05Z</td>\n      <td>13998</td>\n      <td>189</td>\n      <td>UgywoWu8TaqkOzGi8HF4AaABAg</td>\n      <td>Yes</td>\n      <td>Bed Intruder</td>\n      <td>NaT</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>2018</td>\n      <td>07</td>\n      <td>[stream, track, httpsopen.spotify.comalbum6vw3...</td>\n      <td>[stream, proceed, great]</td>\n      <td>[track, httpsopen.spotify.comalbum6vw3oxeqedqc...</td>\n      <td>[track_httpsopen.spotify.comalbum6vw3oxeqedqci...</td>\n      <td>[great_gift]</td>\n      <td>[]</td>\n      <td>[stream, track, httpsopen.spotify.comalbum6vw3...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>j â€¢ d i</td>\n      <td>youtube bros tho ğŸ˜¸ğŸ¤ğŸ¼ğŸ‘ğŸ»ğŸ‘ğŸ»ğŸ‘ğŸ»ğŸ‘ğŸ»ğŸ‘ğŸ»ğŸ‘ğŸ»ğŸ‘ğŸ»ğŸ‘ğŸ»  lol</td>\n      <td>2022-05-15T14:29:59Z</td>\n      <td>0</td>\n      <td>0</td>\n      <td>UgyfeNHiP8lcSxAr2qB4AaABAg</td>\n      <td>Yes</td>\n      <td>Bed Intruder</td>\n      <td>NaT</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>2022</td>\n      <td>05</td>\n      <td>[youtube, bro, tho, lol]</td>\n      <td>[]</td>\n      <td>[youtube, bro, tho, lol]</td>\n      <td>[youtube_bro]</td>\n      <td>[]</td>\n      <td>[]</td>\n      <td>[youtube, bro, tho, lol]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Lonn Roberts</td>\n      <td>this songs been in my head all day</td>\n      <td>2022-05-15T03:42:37Z</td>\n      <td>0</td>\n      <td>0</td>\n      <td>UgxHvi5Eu_iPCzenYkN4AaABAg</td>\n      <td>Yes</td>\n      <td>Bed Intruder</td>\n      <td>NaT</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>2022</td>\n      <td>05</td>\n      <td>[song, head, day]</td>\n      <td>[]</td>\n      <td>[song, head, day]</td>\n      <td>[]</td>\n      <td>[]</td>\n      <td>[]</td>\n      <td>[song, head_day]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Weapon</td>\n      <td>Peak internet glory days</td>\n      <td>2022-05-14T19:55:01Z</td>\n      <td>1</td>\n      <td>0</td>\n      <td>UgwzacFLCUiYgSpm2554AaABAg</td>\n      <td>Yes</td>\n      <td>Bed Intruder</td>\n      <td>NaT</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>2022</td>\n      <td>05</td>\n      <td>[peak, internet, glory, day]</td>\n      <td>[]</td>\n      <td>[peak, internet, glory, day]</td>\n      <td>[internet_glory, internet_glory_day, glory_day]</td>\n      <td>[]</td>\n      <td>[]</td>\n      <td>[peak_internet, glory_day]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>sbi</td>\n      <td>So my question. Did they find the rapist is Li...</td>\n      <td>2022-05-14T16:47:53Z</td>\n      <td>0</td>\n      <td>0</td>\n      <td>UgxxcUMzPyuHt7dpoqB4AaABAg</td>\n      <td>Yes</td>\n      <td>Bed Intruder</td>\n      <td>NaT</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>2022</td>\n      <td>05</td>\n      <td>[question, find, rapist, lincoln, park]</td>\n      <td>[find]</td>\n      <td>[question, rapist, lincoln, park]</td>\n      <td>[]</td>\n      <td>[]</td>\n      <td>[]</td>\n      <td>[question_find, rapist_lincoln, park]</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 22 columns</p>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_comments = pd.read_pickle('../output/07142022_clean_data.pkl')\n",
    "df_comments.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "lemmaSenetence = df_comments.lemmasNgrams.values\n",
    "model = word2vec.Word2Vec(lemmaSenetence, vector_size=50, min_count=3, epochs=20)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "topicSubject =['black','people','violence','rape','ridicule','feminist','critique', 'gay', 'musicality','comic','men']\n",
    "topicBlack = model.wv.most_similar('black', topn=50)\n",
    "topicPeople = model.wv.most_similar('people', topn=50)\n",
    "topicViolence = model.wv.most_similar('violence', topn=50)\n",
    "topicRape = model.wv.most_similar('rape', topn=50)\n",
    "topicRidicule = model.wv.most_similar('ridicule', topn=50)\n",
    "topicFeminist = model.wv.most_similar('feminist', topn=50)\n",
    "topicCritique = model.wv.most_similar('criticize', topn=50)\n",
    "topicGay = model.wv.most_similar('gay', topn=50)\n",
    "topicMusicality = model.wv.most_similar('musicality', topn=50)\n",
    "topicComic = model.wv.most_similar('comic', topn=50)\n",
    "topicMen = model.wv.most_similar('men', topn=50)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "data": {
      "text/plain": "[('white_people', 0.8471476435661316),\n ('race', 0.7865716814994812),\n ('black', 0.754928469657898),\n ('america', 0.7470788955688477),\n ('black_person', 0.7441515326499939),\n ('american', 0.7420827150344849),\n ('racism', 0.7363351583480835),\n ('ignorant', 0.7352720499038696),\n ('human', 0.7341544032096863),\n ('african', 0.7214856147766113),\n ('act_like', 0.7195767760276794),\n ('character', 0.7173447012901306),\n ('racist', 0.7169864773750305),\n ('poor_black', 0.7110163569450378),\n ('mock', 0.7085040807723999),\n ('necessarily', 0.7021762728691101),\n ('speak', 0.6993688344955444),\n ('asshole', 0.6992201805114746),\n ('way_talk', 0.6927571892738342),\n ('white', 0.6921793818473816),\n ('fun', 0.6909670829772949),\n ('understand', 0.679627537727356),\n ('offend', 0.6787835359573364),\n ('wrong', 0.6773495078086853),\n ('folk', 0.673936128616333),\n ('insult', 0.6715297698974609),\n ('white_person', 0.6692882776260376),\n ('act', 0.6691510081291199),\n ('victim', 0.6673231720924377),\n ('americans', 0.6651644110679626),\n ('african_americans', 0.6591861248016357),\n ('assume', 0.6589933633804321),\n ('african_american', 0.6579669117927551),\n ('humor', 0.6578269600868225),\n ('kind', 0.6576917171478271),\n ('country', 0.6547803282737732),\n ('way', 0.6533986330032349),\n ('stereotype', 0.6502553820610046),\n ('reason', 0.6490695476531982),\n ('person', 0.6436980366706848),\n ('ignorance', 0.6421887874603271),\n ('ebonic', 0.6417414546012878),\n ('example', 0.6404320001602173),\n ('make_fun', 0.6401766538619995),\n ('pain', 0.6399849653244019),\n ('fact', 0.6394874453544617),\n ('statement', 0.6381584405899048),\n ('crime', 0.6377338171005249),\n ('lot', 0.6373419761657715),\n ('minority', 0.6358585953712463)]"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('black_people', topn=50)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "topicCols =['topicBlack','topicPeople','topicViolence','topicRape','topicRidicule','topicFeminist','topicCritique', 'topicGay', 'topicMusicality','topicComic','topicMen']\n",
    "for col in topicCols:\n",
    "    df_comments[col] = None\n",
    "\n",
    "topicColsScore = ['topicBlackS','topicPeopleS','topicViolenceS','topicRapeS','topicRidiculeS','topicFeministS','topicCritiqueS', 'topicGayS', 'topicMusicalityS','topicComicS','topicMenS']\n",
    "for col in topicColsScore:\n",
    "    df_comments[col] = None"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "'Exporting top 10 words in each topic to a csv'\n",
    "df=pd.DataFrame()\n",
    "df['topic']=''\n",
    "df['words']=''\n",
    "j=0\n",
    "for col in topicCols:\n",
    "    i=0\n",
    "    for items in eval(col):\n",
    "        #print(items)\n",
    "        df.loc[j,'topic'] = col\n",
    "        df.at[j,'words'] = items\n",
    "        i=i+1\n",
    "        j=j+1\n",
    "        if i ==10:\n",
    "            break\n",
    "df.to_csv(os.path.join('../output/','topic_list_07202022.csv'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "def sim_word(topicName):\n",
    "    \"Extracting only words from similarity output of wordvec\"\n",
    "    simList = set([w[0] for w in topicName if w[1]>.5])\n",
    "    return simList\n",
    "\n",
    "def sim_score(topicName):\n",
    "    \"Extracting word with score from similarity output of wordvec\"\n",
    "    simScore = set([w for w in topicName if w[1]>.5])\n",
    "    return simScore"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "\"Putting all the sim words in one dictionary by topic\"\n",
    "topicDict = {}\n",
    "scoreDict = {}\n",
    "i=0\n",
    "for col in topicCols:\n",
    "    topicDict.update({col: sim_word(eval(col))})\n",
    "    topicDict[col].add(topicSubject[i])\n",
    "    scoreDict.update({col: sim_score(eval(col))})\n",
    "    scoreDict[col].add((topicSubject[i],1))\n",
    "    i=i+1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this songs been in my head all day\n"
     ]
    }
   ],
   "source": [
    "'Sample checks'\n",
    "n=2\n",
    "v=df_comments['lemmasNgrams'][n]\n",
    "#v= ['work', 'screen', 'dirty', 'look_like', 'phone', 'drop', 'coffee', 'stain', 'spot']\n",
    "for topic_, sim_ in topicDict.items():\n",
    "    if len(set(sim_).intersection(set(v)))>0:\n",
    "        common =(set(sim_).intersection(set(v)))\n",
    "        print (topic_, common)\n",
    "        cosScore=0\n",
    "        for items in scoreDict[topic_]:\n",
    "            # Avg. Similarity score\n",
    "            if items[0] in common:\n",
    "                print(items[1])\n",
    "                cosScore = items[1] + cosScore\n",
    "        cosScoreAvg =  cosScore/len(common)\n",
    "        assert cosScoreAvg <= 1\n",
    "        print(cosScoreAvg)\n",
    "\n",
    "print(df_comments['comment'][n])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "'Finding common words between lemmas of review text and most similar words by each topic'\n",
    "df_comments['allTopics']=''\n",
    "for index, row in df_comments.iterrows():\n",
    "    lemmaCheck = row['lemmasNgrams']\n",
    "    for topic_, sim_ in topicDict.items():\n",
    "        common = set(sim_).intersection(set(lemmaCheck))\n",
    "        if len(common)>0:\n",
    "            df_comments.at[index, topic_] = common\n",
    "            df_comments.loc[index, 'allTopics'] = topic_ + ', ' + df_comments.loc[index, 'allTopics']\n",
    "            cosScore=0  # Cosine similarity score\n",
    "            for items in scoreDict[topic_]:\n",
    "                # Avg. Similarity score\n",
    "                if items[0] in common:\n",
    "                    # print(items[1])\n",
    "                    cosScore = items[1] + cosScore\n",
    "            cosScoreAvg =  cosScore/len(common) # Avg. cosine similarity score if there are multiple common words\n",
    "            assert cosScoreAvg <= 1\n",
    "            df_comments.loc[index, (topic_+'S')] = cosScoreAvg\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "allTopics\n                                       199626\ntopicPeople,                            10223\ntopicGay,                                9344\ntopicRape,                               6108\ntopicRape, topicPeople,                  1852\ntopicBlack,                              1532\ntopicGay, topicBlack,                    1026\ntopicPeople, topicBlack,                  970\ntopicGay, topicPeople,                    871\ntopicRape, topicBlack,                    525\ntopicGay, topicRape,                      402\ntopicGay, topicPeople, topicBlack,        367\nName: parent_id, dtype: int64"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_comments.groupby('allTopics')['parent_id'].count().sort_values(ascending=False).head(12)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "df_comments.to_pickle(os.path.join('../output/','all_topic_output_07202022.pkl'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "df_comments.to_csv(os.path.join('../output/','all_topic_output_07202022.csv'),encoding='utf_8_sig')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}