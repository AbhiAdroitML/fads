{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "5Smc7CxVD7E7",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting textacy==0.11.0\r\n",
      "  Downloading textacy-0.11.0-py3-none-any.whl (200 kB)\r\n",
      "\u001B[K     |████████████████████████████████| 200 kB 2.5 MB/s eta 0:00:01\r\n",
      "\u001B[?25hRequirement already satisfied: cytoolz>=0.10.1 in /Users/abhinavbajpai/opt/anaconda3/lib/python3.9/site-packages (from textacy==0.11.0) (0.11.0)\r\n",
      "Requirement already satisfied: scipy>=0.17.0 in /Users/abhinavbajpai/opt/anaconda3/lib/python3.9/site-packages (from textacy==0.11.0) (1.7.3)\r\n",
      "Requirement already satisfied: requests>=2.10.0 in /Users/abhinavbajpai/opt/anaconda3/lib/python3.9/site-packages (from textacy==0.11.0) (2.27.1)\r\n",
      "Requirement already satisfied: scikit-learn>=0.19.0 in /Users/abhinavbajpai/opt/anaconda3/lib/python3.9/site-packages (from textacy==0.11.0) (1.0.2)\r\n",
      "Requirement already satisfied: cachetools>=4.0.0 in /Users/abhinavbajpai/opt/anaconda3/lib/python3.9/site-packages (from textacy==0.11.0) (4.2.2)\r\n",
      "Requirement already satisfied: tqdm>=4.19.6 in /Users/abhinavbajpai/opt/anaconda3/lib/python3.9/site-packages (from textacy==0.11.0) (4.64.0)\r\n",
      "Collecting jellyfish>=0.8.0\r\n",
      "  Downloading jellyfish-0.9.0-cp39-cp39-macosx_10_14_x86_64.whl (25 kB)\r\n",
      "Requirement already satisfied: numpy>=1.17.0 in /Users/abhinavbajpai/opt/anaconda3/lib/python3.9/site-packages (from textacy==0.11.0) (1.21.5)\r\n",
      "Requirement already satisfied: joblib>=0.13.0 in /Users/abhinavbajpai/opt/anaconda3/lib/python3.9/site-packages (from textacy==0.11.0) (1.1.0)\r\n",
      "Collecting spacy>=3.0.0\r\n",
      "  Downloading spacy-3.3.0-cp39-cp39-macosx_10_9_x86_64.whl (6.5 MB)\r\n",
      "\u001B[K     |████████████████████████████████| 6.5 MB 10.7 MB/s eta 0:00:01\r\n",
      "\u001B[?25hCollecting pyphen>=0.10.0\r\n",
      "  Downloading pyphen-0.12.0-py3-none-any.whl (2.0 MB)\r\n",
      "\u001B[K     |████████████████████████████████| 2.0 MB 23.5 MB/s eta 0:00:01\r\n",
      "\u001B[?25hRequirement already satisfied: networkx>=2.0 in /Users/abhinavbajpai/opt/anaconda3/lib/python3.9/site-packages (from textacy==0.11.0) (2.7.1)\r\n",
      "Requirement already satisfied: toolz>=0.8.0 in /Users/abhinavbajpai/opt/anaconda3/lib/python3.9/site-packages (from cytoolz>=0.10.1->textacy==0.11.0) (0.11.2)\r\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/abhinavbajpai/opt/anaconda3/lib/python3.9/site-packages (from requests>=2.10.0->textacy==0.11.0) (2.0.4)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/abhinavbajpai/opt/anaconda3/lib/python3.9/site-packages (from requests>=2.10.0->textacy==0.11.0) (3.3)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/abhinavbajpai/opt/anaconda3/lib/python3.9/site-packages (from requests>=2.10.0->textacy==0.11.0) (2021.10.8)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/abhinavbajpai/opt/anaconda3/lib/python3.9/site-packages (from requests>=2.10.0->textacy==0.11.0) (1.26.9)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/abhinavbajpai/opt/anaconda3/lib/python3.9/site-packages (from scikit-learn>=0.19.0->textacy==0.11.0) (2.2.0)\r\n",
      "Collecting langcodes<4.0.0,>=3.2.0\r\n",
      "  Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\r\n",
      "\u001B[K     |████████████████████████████████| 181 kB 57.2 MB/s eta 0:00:01\r\n",
      "\u001B[?25hRequirement already satisfied: jinja2 in /Users/abhinavbajpai/opt/anaconda3/lib/python3.9/site-packages (from spacy>=3.0.0->textacy==0.11.0) (2.11.3)\r\n",
      "Collecting thinc<8.1.0,>=8.0.14\r\n",
      "  Downloading thinc-8.0.17-cp39-cp39-macosx_10_9_x86_64.whl (645 kB)\r\n",
      "\u001B[K     |████████████████████████████████| 645 kB 53.4 MB/s eta 0:00:01\r\n",
      "\u001B[?25hCollecting wasabi<1.1.0,>=0.9.1\r\n",
      "  Downloading wasabi-0.9.1-py3-none-any.whl (26 kB)\r\n",
      "Collecting preshed<3.1.0,>=3.0.2\r\n",
      "  Downloading preshed-3.0.6-cp39-cp39-macosx_10_9_x86_64.whl (106 kB)\r\n",
      "\u001B[K     |████████████████████████████████| 106 kB 41.7 MB/s eta 0:00:01\r\n",
      "\u001B[?25hCollecting catalogue<2.1.0,>=2.0.6\r\n",
      "  Downloading catalogue-2.0.7-py3-none-any.whl (17 kB)\r\n",
      "Collecting typer<0.5.0,>=0.3.0\r\n",
      "  Downloading typer-0.4.1-py3-none-any.whl (27 kB)\r\n",
      "Requirement already satisfied: setuptools in /Users/abhinavbajpai/opt/anaconda3/lib/python3.9/site-packages (from spacy>=3.0.0->textacy==0.11.0) (61.2.0)\r\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0\r\n",
      "  Downloading spacy_loggers-1.0.2-py3-none-any.whl (7.2 kB)\r\n",
      "Collecting pathy>=0.3.5\r\n",
      "  Downloading pathy-0.6.1-py3-none-any.whl (42 kB)\r\n",
      "\u001B[K     |████████████████████████████████| 42 kB 9.2 MB/s  eta 0:00:01\r\n",
      "\u001B[?25hCollecting spacy-legacy<3.1.0,>=3.0.9\r\n",
      "  Downloading spacy_legacy-3.0.9-py2.py3-none-any.whl (20 kB)\r\n",
      "Collecting srsly<3.0.0,>=2.4.3\r\n",
      "  Downloading srsly-2.4.3-cp39-cp39-macosx_10_9_x86_64.whl (457 kB)\r\n",
      "\u001B[K     |████████████████████████████████| 457 kB 30.0 MB/s eta 0:00:01\r\n",
      "\u001B[?25hCollecting pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4\r\n",
      "  Downloading pydantic-1.8.2-cp39-cp39-macosx_10_9_x86_64.whl (2.7 MB)\r\n",
      "\u001B[K     |████████████████████████████████| 2.7 MB 123.0 MB/s eta 0:00:01\r\n",
      "\u001B[?25hCollecting cymem<2.1.0,>=2.0.2\r\n",
      "  Downloading cymem-2.0.6-cp39-cp39-macosx_10_9_x86_64.whl (32 kB)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/abhinavbajpai/opt/anaconda3/lib/python3.9/site-packages (from spacy>=3.0.0->textacy==0.11.0) (21.3)\r\n",
      "Collecting murmurhash<1.1.0,>=0.28.0\r\n",
      "  Downloading murmurhash-1.0.7-cp39-cp39-macosx_10_9_x86_64.whl (18 kB)\r\n",
      "Collecting blis<0.8.0,>=0.4.0\r\n",
      "  Downloading blis-0.7.7-cp39-cp39-macosx_10_9_x86_64.whl (5.8 MB)\r\n",
      "\u001B[K     |████████████████████████████████| 5.8 MB 14.4 MB/s eta 0:00:01\r\n",
      "\u001B[?25hRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/abhinavbajpai/opt/anaconda3/lib/python3.9/site-packages (from packaging>=20.0->spacy>=3.0.0->textacy==0.11.0) (3.0.4)\r\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /Users/abhinavbajpai/opt/anaconda3/lib/python3.9/site-packages (from pathy>=0.3.5->spacy>=3.0.0->textacy==0.11.0) (5.1.0)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/abhinavbajpai/opt/anaconda3/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy>=3.0.0->textacy==0.11.0) (4.1.1)\r\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/abhinavbajpai/opt/anaconda3/lib/python3.9/site-packages (from typer<0.5.0,>=0.3.0->spacy>=3.0.0->textacy==0.11.0) (8.0.4)\r\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /Users/abhinavbajpai/opt/anaconda3/lib/python3.9/site-packages (from jinja2->spacy>=3.0.0->textacy==0.11.0) (2.0.1)\r\n",
      "Installing collected packages: murmurhash, cymem, catalogue, wasabi, typer, srsly, pydantic, preshed, blis, thinc, spacy-loggers, spacy-legacy, pathy, langcodes, spacy, pyphen, jellyfish, textacy\r\n",
      "Successfully installed blis-0.7.7 catalogue-2.0.7 cymem-2.0.6 jellyfish-0.9.0 langcodes-3.3.0 murmurhash-1.0.7 pathy-0.6.1 preshed-3.0.6 pydantic-1.8.2 pyphen-0.12.0 spacy-3.3.0 spacy-legacy-3.0.9 spacy-loggers-1.0.2 srsly-2.4.3 textacy-0.11.0 thinc-8.0.17 typer-0.4.1 wasabi-0.9.1\r\n"
     ]
    }
   ],
   "source": [
    "!pip install textacy==0.11.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "zAz4bD2-OXVR",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas==1.3.4\r\n",
      "  Downloading pandas-1.3.4-cp39-cp39-macosx_10_9_x86_64.whl (11.6 MB)\r\n",
      "\u001B[K     |████████████████████████████████| 11.6 MB 2.3 MB/s eta 0:00:01\r\n",
      "\u001B[?25hRequirement already satisfied: numpy>=1.17.3 in /Users/abhinavbajpai/opt/anaconda3/lib/python3.9/site-packages (from pandas==1.3.4) (1.21.5)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /Users/abhinavbajpai/opt/anaconda3/lib/python3.9/site-packages (from pandas==1.3.4) (2.8.2)\r\n",
      "Requirement already satisfied: pytz>=2017.3 in /Users/abhinavbajpai/opt/anaconda3/lib/python3.9/site-packages (from pandas==1.3.4) (2021.3)\r\n",
      "Requirement already satisfied: six>=1.5 in /Users/abhinavbajpai/opt/anaconda3/lib/python3.9/site-packages (from python-dateutil>=2.7.3->pandas==1.3.4) (1.16.0)\r\n",
      "Installing collected packages: pandas\r\n",
      "  Attempting uninstall: pandas\r\n",
      "    Found existing installation: pandas 1.4.2\r\n",
      "    Uninstalling pandas-1.4.2:\r\n",
      "      Successfully uninstalled pandas-1.4.2\r\n",
      "Successfully installed pandas-1.3.4\r\n"
     ]
    }
   ],
   "source": [
    "!pip3 install pandas==1.3.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L_-UVFuCPT4U",
    "outputId": "cc8d9f10-5fab-452e-c113-2079614170c5",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xlsxwriter in /Users/abhinavbajpai/opt/anaconda3/lib/python3.9/site-packages (3.0.3)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install xlsxwriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm &>devnull"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting WordCloud\r\n",
      "  Downloading wordcloud-1.8.1.tar.gz (220 kB)\r\n",
      "\u001B[K     |████████████████████████████████| 220 kB 2.4 MB/s eta 0:00:01\r\n",
      "\u001B[?25hRequirement already satisfied: numpy>=1.6.1 in /Users/abhinavbajpai/opt/anaconda3/lib/python3.9/site-packages (from WordCloud) (1.21.5)\r\n",
      "Requirement already satisfied: pillow in /Users/abhinavbajpai/opt/anaconda3/lib/python3.9/site-packages (from WordCloud) (9.0.1)\r\n",
      "Requirement already satisfied: matplotlib in /Users/abhinavbajpai/opt/anaconda3/lib/python3.9/site-packages (from WordCloud) (3.5.1)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/abhinavbajpai/opt/anaconda3/lib/python3.9/site-packages (from matplotlib->WordCloud) (1.3.2)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/abhinavbajpai/opt/anaconda3/lib/python3.9/site-packages (from matplotlib->WordCloud) (2.8.2)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/abhinavbajpai/opt/anaconda3/lib/python3.9/site-packages (from matplotlib->WordCloud) (0.11.0)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/abhinavbajpai/opt/anaconda3/lib/python3.9/site-packages (from matplotlib->WordCloud) (4.25.0)\r\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /Users/abhinavbajpai/opt/anaconda3/lib/python3.9/site-packages (from matplotlib->WordCloud) (3.0.4)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/abhinavbajpai/opt/anaconda3/lib/python3.9/site-packages (from matplotlib->WordCloud) (21.3)\r\n",
      "Requirement already satisfied: six>=1.5 in /Users/abhinavbajpai/opt/anaconda3/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib->WordCloud) (1.16.0)\r\n",
      "Building wheels for collected packages: WordCloud\r\n",
      "  Building wheel for WordCloud (setup.py) ... \u001B[?25ldone\r\n",
      "\u001B[?25h  Created wheel for WordCloud: filename=wordcloud-1.8.1-cp39-cp39-macosx_10_9_x86_64.whl size=154343 sha256=61e0eaeed78632f22789c23a0271800ebb01a8f054fd23258f62ff9f92b3a9b8\r\n",
      "  Stored in directory: /Users/abhinavbajpai/Library/Caches/pip/wheels/f9/7a/dd/06ef8b5dfe5483f6204133c08eeb16c287cc2c05e290ae2fc0\r\n",
      "Successfully built WordCloud\r\n",
      "Installing collected packages: WordCloud\r\n",
      "Successfully installed WordCloud-1.8.1\r\n"
     ]
    }
   ],
   "source": [
    "!pip install WordCloud"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "import spacy\n",
    "import textacy\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import html\n",
    "import xlsxwriter\n",
    "from collections import Counter\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "lRojtYF6DXrl",
    "outputId": "2de18c5c-f979-4d67-e708-ed4b80fe3c7b",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                 name                                            comment  \\\n1           schmoyoho  Stream the track! https://open.spotify.com/alb...   \n2             Roweman              I wonder if they caught that intruder   \n3        xxCordellMxx    Why did I say this out loud when I was younger?   \n4  Nicholas DiGaetano                       The rapist was never caught?   \n5         Tango Bodhi                           This will never get old.   \n\n           published_at  likes Reply Count                   parent_id  \\\n1  2018-07-12T18:42:05Z  13950         188  UgywoWu8TaqkOzGi8HF4AaABAg   \n2  2022-04-23T00:46:20Z      1           0  UgyUDUe6xUHoX5ziaql4AaABAg   \n3  2022-04-21T20:27:23Z      0           0  Ugx9p0We_6_2c5etHQZ4AaABAg   \n4  2022-04-21T04:17:40Z      0           0  Ugz-Y1LK6QpBOVv2WI54AaABAg   \n5  2022-04-20T18:27:49Z      0           0  UgyHtE_H7Y5OOda5gfZ4AaABAg   \n\n  is_parent  \n1       Yes  \n2       Yes  \n3       Yes  \n4       Yes  \n5       Yes  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>name</th>\n      <th>comment</th>\n      <th>published_at</th>\n      <th>likes</th>\n      <th>Reply Count</th>\n      <th>parent_id</th>\n      <th>is_parent</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>schmoyoho</td>\n      <td>Stream the track! https://open.spotify.com/alb...</td>\n      <td>2018-07-12T18:42:05Z</td>\n      <td>13950</td>\n      <td>188</td>\n      <td>UgywoWu8TaqkOzGi8HF4AaABAg</td>\n      <td>Yes</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Roweman</td>\n      <td>I wonder if they caught that intruder</td>\n      <td>2022-04-23T00:46:20Z</td>\n      <td>1</td>\n      <td>0</td>\n      <td>UgyUDUe6xUHoX5ziaql4AaABAg</td>\n      <td>Yes</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>xxCordellMxx</td>\n      <td>Why did I say this out loud when I was younger?</td>\n      <td>2022-04-21T20:27:23Z</td>\n      <td>0</td>\n      <td>0</td>\n      <td>Ugx9p0We_6_2c5etHQZ4AaABAg</td>\n      <td>Yes</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Nicholas DiGaetano</td>\n      <td>The rapist was never caught?</td>\n      <td>2022-04-21T04:17:40Z</td>\n      <td>0</td>\n      <td>0</td>\n      <td>Ugz-Y1LK6QpBOVv2WI54AaABAg</td>\n      <td>Yes</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Tango Bodhi</td>\n      <td>This will never get old.</td>\n      <td>2022-04-20T18:27:49Z</td>\n      <td>0</td>\n      <td>0</td>\n      <td>UgyHtE_H7Y5OOda5gfZ4AaABAg</td>\n      <td>Yes</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'Change path to data folder'\n",
    "path = '../data/05202022_youtube_comments.pkl'\n",
    "df = pd.read_pickle(path)\n",
    "df = df.drop(0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "sTgXw2SQDa7K",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "'Declaring Text Cleaning Functions'\n",
    "RE_SUSPICIOUS = re.compile(r'[&#<>{}\\[\\]\\\\]')\n",
    "def impurity(text, min_len=10):\n",
    "    # returns the share of suspicious characters in a text\n",
    "    if text == None or len(text) < min_len:\n",
    "        return 0\n",
    "    else:\n",
    "        return len(RE_SUSPICIOUS.findall(text)) / len(text)\n",
    "\n",
    "def clean(text):\n",
    "    # convert html escapes like &amp; to characters.\n",
    "    text = html.unescape(text)\n",
    "    # tags like <tab>\n",
    "    text = re.sub(r'<[^<>]*>', ' ', text)\n",
    "    # markdown URLs like [Some text](https://....)\n",
    "    text = re.sub(r'\\[([^\\[\\]]*)\\]\\([^\\(\\)]*\\)', r'\\1', text)\n",
    "    # text or code in brackets like [0]\n",
    "    text = re.sub(r'\\[[^\\[\\]]*\\]', ' ', text)\n",
    "    # standalone sequences of specials, matches &# but not #cool\n",
    "    text = re.sub(r'(?:^|\\s)[&#<>{}\\[\\]+|\\\\:-]{1,}(?:\\s|$)', ' ', text)\n",
    "    # standalone sequences of hyphens like --- or ==\n",
    "    text = re.sub(r'(?:^|\\s)[\\-=\\+]{2,}(?:\\s|$)', ' ', text)\n",
    "    # sequences of white spaces\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    # remove >>\n",
    "    text = re.sub(r\">\\S+\", ' ', text)\n",
    "    # remove #\n",
    "    text = re.sub(r\"#\\S+\", ' ', text)\n",
    "    # remove <<\n",
    "    text = re.sub(r\"<\\S+\", ' ', text)\n",
    "    return text.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "ot6KjCEnIAv3",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "'Additional Text cleaning Using Textacy'\n",
    "import textacy.preprocessing as tprep\n",
    "def normalize(text):\n",
    " 'Removes additional text impurities using built-in textacy functions'\n",
    " text = tprep.normalize.hyphenated_words(text)\n",
    " text = tprep.normalize.quotation_marks(text)\n",
    " text = tprep.normalize.unicode(text)\n",
    " text = tprep.remove.accents(text)\n",
    " text = tprep.replace.urls(text, repl = '')\n",
    " text = tprep.replace.emails(text, repl = '') \n",
    " text = tprep.replace.hashtags(text, repl = '') \n",
    " text = tprep.replace.numbers(text, repl = '') \n",
    " text = tprep.replace.phone_numbers(text, repl = '') \n",
    " text = tprep.replace.user_handles(text, repl ='') \n",
    " text = tprep.replace.emojis(text, repl = '')\n",
    " return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "ZwrjbHYwDf08",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Applying clean and impurity function\n",
    "df['comment_lower'] = df['comment'].str.lower()\n",
    "df['clean_comments'] = df['comment_lower'].apply(lambda x: clean(str(x)))\n",
    "df['clean_comments'] = df['clean_comments'].map(normalize)\n",
    "df['impurity'] = df['clean_comments'].apply(impurity, min_len=20)\n",
    "df[['clean_comments', 'impurity']].sort_values(by='impurity', ascending=False).head(20) \n",
    "df['year'] = df['published_at'].apply(lambda x : x[:4])\n",
    "df['month'] = df['published_at'].apply(lambda x : x[5:7])\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hVDu0q52Gy1g",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Lemmatization Using Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "w80hZ-kyYYAp",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"ner\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "-GNs2Gvuf9_a",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def extract_lemmas(doc, **kwargs):\n",
    "    ''' Extract Lemmas '''\n",
    "    extractedLemma = [t.lemma_ for t in textacy.extract.words(doc, **kwargs)]\n",
    "    return extractedLemma\n",
    "\n",
    "def extract_noun_phrases(doc, preceding_pos=['NOUN'], sep='_'):\n",
    "    '''Extract Noun Phrases '''\n",
    "    patterns = []\n",
    "    for pos in preceding_pos:\n",
    "        patterns.append(f\"POS:{pos} POS:NOUN:+\")\n",
    "    spans = textacy.extract.matches.token_matches(doc, patterns=patterns)\n",
    "    nounPhrase = [sep.join([t.lemma_ for t in s]) for s in spans]    \n",
    "    return nounPhrase\n",
    "\n",
    "def extract_nlp(doc):\n",
    "    ''' Extract Various Combinations of POS'''\n",
    "    posResults= {\n",
    "        'lemmas': extract_lemmas(doc,\n",
    "                                  exclude_pos=['PART', 'PUNCT',\n",
    "                                             'DET', 'PRON', 'SYM', 'SPACE'],\n",
    "                                filter_stops=True),\n",
    "        'adjs_verbs': extract_lemmas(doc, include_pos=['ADJ', 'VERB']),\n",
    "        'nouns': extract_lemmas(doc, include_pos=['NOUN', 'PROPN']),\n",
    "        'noun_phrases': extract_noun_phrases(doc, ['NOUN']),\n",
    "        'adj_noun_phrases': extract_noun_phrases(doc, ['ADJ']),\n",
    "        'Verb_noun_phrases': extract_noun_phrases(doc, ['VERB'])}\n",
    "    return posResults\n",
    "\n",
    "nlp_columns = list(extract_nlp(nlp.make_doc('')).keys())\n",
    "\n",
    "for col in nlp_columns:\n",
    "    df[col] = None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9NMGPveML6Ed",
    "outputId": "1de446c1-4040-44bd-86d7-ed72cd9309b9",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/abhinavbajpai/opt/anaconda3/lib/python3.9/site-packages/pandas/core/indexing.py:1732: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_block(indexer, value, name)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "for i in range(0, len(df), batch_size):\n",
    "    docs = nlp.pipe(df['clean_comments'][i:i + batch_size])\n",
    "    for j, doc in enumerate(docs):\n",
    "        for col, values in extract_nlp(doc).items():\n",
    "            df[col].iloc[i + j] = values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mnygLoe5S7em",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Creating N-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "uS6ittSUMFKQ",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sentences = df.lemmas.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "IPZomf_JMHq7",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.models import Phrases\n",
    "from gensim.models.phrases import Phraser\n",
    "bigram = Phrases(sentences, min_count=1, threshold=3, delimiter='_')\n",
    "bigram_phraser = Phraser(bigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "O-lPHZk_KENk",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df['lemmasNgrams']=None\n",
    "i=0\n",
    "for sent in sentences:\n",
    "  df.at[i, 'lemmasNgrams'] = bigram_phraser[sent]\n",
    "  i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "VkP5ykNHLV-r",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "                                              lemmas  \\\n0  [stream, track, proceed, dodson, gregorys, ant...   \n1                          [wonder, catch, intruder]   \n2                                      [loud, young]   \n3                                    [rapist, catch]   \n4                                              [old]   \n\n                                        lemmasNgrams  \n0  [stream, track, proceed_dodson, gregorys, anto...  \n1                           [wonder_catch, intruder]  \n2                                      [loud, young]  \n3                                    [rapist, catch]  \n4                                              [old]  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>lemmas</th>\n      <th>lemmasNgrams</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>[stream, track, proceed, dodson, gregorys, ant...</td>\n      <td>[stream, track, proceed_dodson, gregorys, anto...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>[wonder, catch, intruder]</td>\n      <td>[wonder_catch, intruder]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>[loud, young]</td>\n      <td>[loud, young]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>[rapist, catch]</td>\n      <td>[rapist, catch]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>[old]</td>\n      <td>[old]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['lemmas','lemmasNgrams']].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "47u_Xt-gtXy0",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df.to_pickle('../output/06062022_clean_data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "Lltn0HfTO7nO",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df.to_excel('../output/06062022_clean_data.xlsx', index= False, engine='xlsxwriter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "18ftCfjLU3t5",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# df = pd.read_pickle('../output/06062022_clean_data.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Count words by Frequency"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "_i4Z5d5UIH9_",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Function to count words\n",
    "def count_words(df, column= col, preprocess=None, min_freq=2):\n",
    "  # process tokens and update counter\n",
    "  def update(doc):\n",
    "    tokens = doc if preprocess is None else preprocess(doc)\n",
    "    counter.update(tokens)\n",
    "  # create counter and run through all data\n",
    "  counter = Counter()\n",
    "  df[column].map(update)\n",
    "  # transform counter into a DataFrame\n",
    "  freq_df = pd.DataFrame.from_dict(counter, orient='index', columns=['freq'])\n",
    "  freq_df = freq_df.query('freq >= @min_freq')\n",
    "  freq_df.index.name = 'token'\n",
    "  return freq_df.sort_values('freq', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "FVCbu2p_1PFP",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_all_years = []\n",
    "df['month'] = df['published_at'].apply(lambda x : x[5:7])\n",
    "for year in df.year.unique():\n",
    "  for month in df.month.unique():    \n",
    "    df_year = df[(df['year']==year) & (df['month']==month)] \n",
    "    freq_df = count_words(df_year, column= 'lemmas')\n",
    "    freq_df['year'] = year\n",
    "    freq_df['month']= month\n",
    "    freq_df['comment_count']= len(df_year)\n",
    "    \n",
    "    freq_df = freq_df.reset_index(drop=False)\n",
    "    \n",
    "    df_all_years.append(freq_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "lUkXm7sk53po",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_all_years = pd.concat(df_all_years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "2cg2ohxwgewT",
    "outputId": "f68dab76-2a8b-4aa8-89da-21abb7cd8f5a",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "          token freq  year month  comment_count\n0          year   28  2018    07            327\n1          song   22  2018    07            327\n2          july   21  2018    07            327\n3         watch   21  2018    07            327\n4           lol   16  2018    07            327\n...         ...  ...   ...   ...            ...\n4157       duck    2  2010    08          24433\n4158      legal    2  2010    08          24433\n4159      tweet    2  2010    08          24433\n4160    dislick    2  2010    08          24433\n4161  conscious    2  2010    08          24433\n\n[48671 rows x 5 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>token</th>\n      <th>freq</th>\n      <th>year</th>\n      <th>month</th>\n      <th>comment_count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>year</td>\n      <td>28</td>\n      <td>2018</td>\n      <td>07</td>\n      <td>327</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>song</td>\n      <td>22</td>\n      <td>2018</td>\n      <td>07</td>\n      <td>327</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>july</td>\n      <td>21</td>\n      <td>2018</td>\n      <td>07</td>\n      <td>327</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>watch</td>\n      <td>21</td>\n      <td>2018</td>\n      <td>07</td>\n      <td>327</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>lol</td>\n      <td>16</td>\n      <td>2018</td>\n      <td>07</td>\n      <td>327</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>4157</th>\n      <td>duck</td>\n      <td>2</td>\n      <td>2010</td>\n      <td>08</td>\n      <td>24433</td>\n    </tr>\n    <tr>\n      <th>4158</th>\n      <td>legal</td>\n      <td>2</td>\n      <td>2010</td>\n      <td>08</td>\n      <td>24433</td>\n    </tr>\n    <tr>\n      <th>4159</th>\n      <td>tweet</td>\n      <td>2</td>\n      <td>2010</td>\n      <td>08</td>\n      <td>24433</td>\n    </tr>\n    <tr>\n      <th>4160</th>\n      <td>dislick</td>\n      <td>2</td>\n      <td>2010</td>\n      <td>08</td>\n      <td>24433</td>\n    </tr>\n    <tr>\n      <th>4161</th>\n      <td>conscious</td>\n      <td>2</td>\n      <td>2010</td>\n      <td>08</td>\n      <td>24433</td>\n    </tr>\n  </tbody>\n</table>\n<p>48671 rows × 5 columns</p>\n</div>"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all_years.head(-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "rMDl3Kmh8JKh",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_all_years.to_csv(os.path.join('../output/', '06062022_words_by_years.csv')) # Output for Tableau Reports"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "FADS_Text_Analysis_05302022",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}